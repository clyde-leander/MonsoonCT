{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a90989-ede7-44fd-a1ce-60dd8abd4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import groupby\n",
    "\n",
    "def load_data(accounts_path, enquiry_path, flags_path):\n",
    "    print('loading data')\n",
    "    def my_json_file_reader(path):\n",
    "        with open(path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        flattened_data = [item for sublist in json_data for item in sublist]\n",
    "        return pd.json_normalize(flattened_data)\n",
    "\n",
    "    accounts_df = my_json_file_reader(accounts_path)\n",
    "    enquiry_df = my_json_file_reader(enquiry_path)\n",
    "    flags_df = pd.read_csv(flags_path)\n",
    "    \n",
    "    return accounts_df, enquiry_df, flags_df\n",
    "\n",
    "def preprocess_accounts(accounts_df):\n",
    "    print('preprocessing acc data')\n",
    "    accounts_df['open_date'] = pd.to_datetime(accounts_df['open_date'], errors='coerce')\n",
    "    accounts_df['closed_date'] = pd.to_datetime(accounts_df['closed_date'], errors='coerce')\n",
    "\n",
    "    accounts_df['payment_str_missing'] = accounts_df['payment_hist_string'].apply(\n",
    "        lambda x: 1 if pd.isnull(x) or x == '' else 0\n",
    "    )\n",
    "\n",
    "    accounts_df['is_account_open'] = accounts_df['closed_date'].isna().astype(int)\n",
    "    accounts_df['payment_hist_string'] = np.where(\n",
    "        accounts_df['payment_str_missing'] == 1, \n",
    "        '000', \n",
    "        accounts_df['payment_hist_string']\n",
    "    )\n",
    "    accounts_df['Mob1'] = np.where(\n",
    "    (accounts_df['payment_str_missing'] == 1)&(accounts_df['is_account_open']==0), \n",
    "    1, \n",
    "    0)\n",
    "    \n",
    "    return accounts_df\n",
    "\n",
    "def preprocess_enquiries(enquiry_df, reference_date):\n",
    "    print('preprocessing enq data')\n",
    "    enquiry_df['enquiry_date'] = pd.to_datetime(enquiry_df['enquiry_date'], errors='coerce')\n",
    "    enquiry_df['in_last_quarter'] = (enquiry_df['enquiry_date'] > (reference_date - pd.DateOffset(months=3)))\n",
    "    enquiry_df['in_last_six_months'] = (enquiry_df['enquiry_date'] > (reference_date - pd.DateOffset(months=6)))\n",
    "    enquiry_df['in_last_month'] = (enquiry_df['enquiry_date'] > (reference_date - pd.DateOffset(months=1)))\n",
    "    \n",
    "    return enquiry_df\n",
    "\n",
    "def extract_payment_features(payment_hist_string):\n",
    "    months = [payment_hist_string[i:i+3] for i in range(0, len(payment_hist_string), 3)]\n",
    "    overdue_days = [int(month) for month in months]\n",
    "    \n",
    "    on_time_count = sum(1 for days in overdue_days if days == 0)\n",
    "    max_days_overdue = max(overdue_days)\n",
    "    average_days_overdue = sum(overdue_days) / len(overdue_days)\n",
    "    months_late_count = sum(1 for days in overdue_days if days > 0)\n",
    "    \n",
    "    last_six_months = overdue_days[-6:] if len(overdue_days) >= 6 else overdue_days\n",
    "    last_six_percent = sum(1 for days in last_six_months if days > 0)/6\n",
    "    \n",
    "    last_three_months = overdue_days[-3:] if len(overdue_days) >= 3 else overdue_days\n",
    "    last_three_percent = sum(1 for days in last_three_months if days > 0)/3\n",
    "    \n",
    "    ever_0_by_3= 1 if sum(1 for days in last_three_months if days > 0)==0 else 0\n",
    "    ever_1_by_3= 1 if sum(1 for days in last_three_months if days > 0)==1 else 0\n",
    "    ever_2_by_3= 1 if sum(1 for days in last_three_months if days > 0)==2 else 0\n",
    "    ever_3_by_3= 1 if sum(1 for days in last_three_months if days > 0)==3 else 0\n",
    "    ever_30_plus = 1 if any(days > 30 for days in overdue_days) else 0\n",
    "    longest_streak=max((len(g) for g in [list(g) for k, g in groupby(overdue_days) if k == 0]), default=0)\n",
    "    \n",
    "    return pd.Series([on_time_count, max_days_overdue, average_days_overdue, months_late_count,last_six_percent,last_three_percent,\n",
    "                      \n",
    "                      on_time_count+months_late_count,ever_0_by_3,ever_1_by_3,ever_2_by_3,ever_3_by_3,ever_30_plus,longest_streak])\n",
    "\n",
    "def engineer_features(accounts_df, enquiry_df, reference_date):\n",
    "    print('aggregation of data')\n",
    "    tqdm.pandas(desc=\"Processing payment history\")\n",
    "    accounts_df[['on_time_count', 'max_days_overdue', 'average_days_overdue', \n",
    "                        'months_late_count','last_six_percent','last_three_percent','total_months',\n",
    "                       'ever_0_by_3','ever_1_by_3','ever_2_by_3'\n",
    "                 ,'ever_3_by_3','ever_30_plus','longest_streak']]= accounts_df['payment_hist_string'].progress_apply(extract_payment_features)\n",
    "    print(\"pay hist str processing completed.\")\n",
    "    accounts_df['Never Del']= np.where(accounts_df['on_time_count'] == accounts_df['total_months'], 1, 0)\n",
    "\n",
    "    open_accounts = accounts_df[accounts_df['is_account_open'] == 1]\n",
    "    closed_accounts = accounts_df[accounts_df['is_account_open'] == 0]\n",
    "    closed_accounts['loan_duration']=(closed_accounts['closed_date'] - closed_accounts['open_date']).dt.days\n",
    "\n",
    "    print(\"open acc.\")\n",
    "\n",
    "\n",
    "    open_accounts_agg = open_accounts.groupby('uid').agg({\n",
    "        'uid':['count'],\n",
    "        'loan_amount': ['sum', 'mean', 'max'],  \n",
    "        'amount_overdue': ['sum', 'mean'], \n",
    "        'on_time_count':['sum'],\n",
    "        'months_late_count':['sum'],\n",
    "        'max_days_overdue':['max'], \n",
    "        'average_days_overdue':['mean'],\n",
    "        'last_six_percent':['max'],\n",
    "        'last_three_percent':['max'],\n",
    "        'total_months':['mean', 'max'],\n",
    "            'ever_0_by_3':['sum'],\n",
    "        'ever_1_by_3':['sum'],'ever_2_by_3':['sum'],'ever_3_by_3':['sum'],'ever_30_plus':['sum'],'longest_streak':['mean'],\n",
    "        'Never Del':['sum'],'Mob1':['sum']\n",
    "        \n",
    "    }).reset_index()\n",
    "    \n",
    "    \n",
    "    open_accounts_agg.columns = ['uid',\n",
    "     'open_count',\n",
    "     'open_total_loan_amount',\n",
    "     'open_mean_loan_amount',\n",
    "     'open_max_loan_amount',\n",
    "     'open_total_amount_overdue',\n",
    "     'open_mean_amount_overdue',\n",
    "     'open_total_on_time_count',\n",
    "     'open_total_late_count',\n",
    "     'open_max_days_overdue',\n",
    "     'open_average_days_overdue',\n",
    "    'open_last_six_percent',\n",
    "    'open_last_three_percent',\n",
    "    'open_mean_loan_duration',\n",
    "    'open_max_loan_duration',\n",
    "         'open_ever_0_by_3','open_ever_1_by_3','open_ever_2_by_3','open_ever_3_by_3','open_ever_30_plus','open_longest_streak',\n",
    "    'open_Never_del','open_Mob1']\n",
    "                           \n",
    "    print(\"closed acc.\")\n",
    "                        \n",
    "                                \n",
    "    \n",
    "    closed_accounts_agg = closed_accounts.groupby('uid').agg({\n",
    "        'uid':['count'],\n",
    "        'loan_amount': ['sum', 'mean', 'max'], \n",
    "        'amount_overdue': ['sum', 'mean'],  \n",
    "        'on_time_count':['sum'],\n",
    "        'months_late_count':['sum'],\n",
    "        'max_days_overdue':['max'], \n",
    "        'average_days_overdue':['mean'],\n",
    "        'loan_duration':['mean', 'max'],\n",
    "            'ever_0_by_3':['sum'],\n",
    "        'ever_1_by_3':['sum'],'ever_2_by_3':['sum'],'ever_3_by_3':['sum'],'ever_30_plus':['sum'],'longest_streak':['mean'],\n",
    "        'Never Del':['sum'],'Mob1':['sum']\n",
    "    }).reset_index()\n",
    "    \n",
    "    closed_accounts_agg.columns = ['uid',\n",
    "    'closed_count',\n",
    "     'closed_total_loan_amount',\n",
    "     'closed_mean_loan_amount',\n",
    "     'closed_max_loan_amount',\n",
    "     'closed_total_amount_overdue',\n",
    "     'closed_mean_amount_overdue',\n",
    "     'closed_total_on_time_count',\n",
    "     'closed_total_late_count',\n",
    "     'closed_max_days_overdue',\n",
    "     'closed_average_days_overdue',\n",
    "    'closed_mean_loan_duration',\n",
    "    'closed_max_loan_duration',\n",
    "         'closed_ever_0_by_3','closed_ever_1_by_3','closed_ever_2_by_3','closed_ever_3_by_3','closed_ever_30_plus','closed_longest_streak',\n",
    "    'closed_Never_del','closed_Mob1'\n",
    "    \n",
    "                                ]\n",
    "    \n",
    "    open_accounts_agg['open_on_time_percent']=open_accounts_agg['open_total_on_time_count']/(open_accounts_agg['open_total_on_time_count']+open_accounts_agg['open_total_late_count'])\n",
    "    closed_accounts_agg['closed_on_time_percent']=closed_accounts_agg['closed_total_on_time_count']/(closed_accounts_agg['closed_total_on_time_count']+closed_accounts_agg['closed_total_late_count'])\n",
    "    accounts_agg = open_accounts_agg.merge(closed_accounts_agg, on='uid', how='outer')\n",
    "\n",
    "    print(\"grouping enq\")\n",
    "\n",
    "    enquiry_agg = enquiry_df.groupby('uid').agg({\n",
    "    'enquiry_amt': ['sum', 'mean', 'max', 'count'],  \n",
    "    'enquiry_type': ['nunique'],  \n",
    "        'in_last_quarter': ['sum'],  \n",
    "    'in_last_six_months': ['sum'],\n",
    "    'in_last_month':['sum']\n",
    "    }).reset_index()\n",
    "\n",
    "    enquiry_agg.columns = [\n",
    "        'uid', \n",
    "        'enquiry_amt_sum', \n",
    "        'enquiry_amt_mean', \n",
    "        'enquiry_amt_max', \n",
    "        'enquiry_count', \n",
    "        'unique_enquiry_types', \n",
    "        'last_quarter_enquiries', \n",
    "        'last_six_months_enquiries',\n",
    "        'last_month_enquiries'\n",
    "    ]\n",
    "\n",
    "    return accounts_agg, enquiry_agg\n",
    "\n",
    "def calculate_reference_date(accounts_df):\n",
    "    #I believe I need a reference date against which I can check the enquiry date and such features\n",
    "    #I think this is old data and using today's date will be wrong\n",
    "    #therefore I am going to use the below date which is calculated using the open date and length of the payment history string\n",
    "    print('calculating ref date')\n",
    "    random_sample_df = accounts_df[accounts_df['is_account_open'] == 1].sample(n=5000, random_state=1)\n",
    "    random_sample_df['count_months'] = random_sample_df['payment_hist_string'].str.len() / 3\n",
    "    random_sample_df['ref_date'] = random_sample_df.apply(\n",
    "        lambda row: row['open_date'] + pd.DateOffset(months=row['count_months']),\n",
    "        axis=1\n",
    "    )\n",
    "    reference_date = max(random_sample_df['ref_date'])\n",
    "    return reference_date\n",
    "\n",
    "def train_pipeline(accounts_path, enquiry_path, flags_path):\n",
    "    \n",
    "    accounts_df, enquiry_df, flags_df = load_data(accounts_path, enquiry_path, flags_path)\n",
    "    accounts_df = preprocess_accounts(accounts_df)\n",
    "    \n",
    "    reference_date = calculate_reference_date(accounts_df)\n",
    "    enquiry_df = preprocess_enquiries(enquiry_df, reference_date)\n",
    "    \n",
    "    accounts_agg, enquiry_agg = engineer_features(accounts_df, enquiry_df, reference_date)\n",
    "    \n",
    "    training_df = flags_df.merge(accounts_agg, on='uid', how='left')\n",
    "    training_df = training_df.merge(enquiry_agg, on='uid', how='left')\n",
    "\n",
    "\n",
    "    encoder = OneHotEncoder(sparse=False, drop='first') \n",
    "    encoded_features = encoder.fit_transform(training_df[['NAME_CONTRACT_TYPE']])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['NAME_CONTRACT_TYPE']))\n",
    "    \n",
    "    df_encoded = pd.concat([training_df, encoded_df], axis=1)\n",
    "    \n",
    "    df_encoded.drop(columns=['NAME_CONTRACT_TYPE'], inplace=True)\n",
    "    \n",
    "    df_encoded['no_closed_loans'] = np.where(df_encoded['closed_count'].isna(), 1, 0)\n",
    "    \n",
    "    df_encoded['no_open_loans'] = np.where(df_encoded['open_count'].isna(), 1, 0)\n",
    "    \n",
    "    df_encoded.fillna(0.0001, inplace=True)\n",
    "\n",
    "    \n",
    "    X = df_encoded.drop(columns=['TARGET', 'uid'])\n",
    "    y = df_encoded['TARGET']\n",
    "\n",
    "    print('handling outliers')\n",
    "    for col in X.columns:\n",
    "        Q1 = X[col].quantile(0.25)\n",
    "        Q3 = X[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        X[col] = np.where(X[col] > upper_bound, upper_bound, X[col])\n",
    "        X[col] = np.where(X[col] < lower_bound, lower_bound, X[col])\n",
    "        X[col]=np.cbrt(X[col])\n",
    "\n",
    "    \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    model = LGBMClassifier(random_state=42, scale_pos_weight=len(y_train) / sum(y_train))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    print(f\"Training ROC AUC: {roc_auc}\")\n",
    "    \n",
    "    return model, scaler,encoder, reference_date  \n",
    "\n",
    "\n",
    "def test_pipeline(accounts_path, enquiry_path, flags_path, model, scaler, encoder, reference_date):\n",
    "    accounts_df, enquiry_df, flags_df = load_data(accounts_path, enquiry_path, flags_path)\n",
    "    accounts_df = preprocess_accounts(accounts_df)\n",
    "    \n",
    "    enquiry_df = preprocess_enquiries(enquiry_df, reference_date)\n",
    "\n",
    "    accounts_agg, enquiry_agg = engineer_features(accounts_df, enquiry_df, reference_date)\n",
    "    \n",
    "    test_df = flags_df.merge(accounts_agg, on='uid', how='left')\n",
    "    test_df = test_df.merge(enquiry_agg, on='uid', how='left')\n",
    "\n",
    "    encoded_features = encoder.transform(test_df[['NAME_CONTRACT_TYPE']])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['NAME_CONTRACT_TYPE']))\n",
    "    \n",
    "    df_encoded = pd.concat([test_df, encoded_df], axis=1)\n",
    "\n",
    "    df_encoded.drop(columns=['NAME_CONTRACT_TYPE'], inplace=True)\n",
    "\n",
    "    df_encoded['no_closed_loans'] = np.where(df_encoded['closed_count'].isna(), 1, 0)\n",
    "    df_encoded['no_open_loans'] = np.where(df_encoded['open_count'].isna(), 1, 0)\n",
    "\n",
    "    df_encoded.fillna(0.0001, inplace=True)\n",
    "\n",
    "    X_test = df_encoded.drop(columns=['uid'])\n",
    "\n",
    "    for col in X_test.columns:\n",
    "        Q1 = X_test[col].quantile(0.25)\n",
    "        Q3 = X_test[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        X_test[col] = np.where(X_test[col] > upper_bound, upper_bound, X_test[col])\n",
    "        X_test[col] = np.where(X_test[col] < lower_bound, lower_bound, X_test[col])\n",
    "        X_test[col] = np.cbrt(X_test[col])\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "    y_test_pred = model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "\n",
    "    test_df['pred'] = y_test_pred\n",
    "    return test_df[['uid','pred']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9c127f-59c4-4b5c-8b6c-a6b2fa402546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preprocessing acc data\n",
      "calculating ref date\n",
      "preprocessing enq data\n",
      "aggregation of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing payment history: 100%|██████████████████████████████████████████| 1245310/1245310 [12:12<00:00, 1699.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pay hist str processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CLD\\AppData\\Local\\Temp\\ipykernel_29412\\1234274756.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  closed_accounts['loan_duration']=(closed_accounts['closed_date'] - closed_accounts['open_date']).dt.days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open acc.\n",
      "closed acc.\n",
      "grouping enq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CLD\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling outliers\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 16846, number of negative: 192260\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4658\n",
      "[LightGBM] [Info] Number of data points in the train set: 209106, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080562 -> initscore=-2.434735\n",
      "[LightGBM] [Info] Start training from score -2.434735\n",
      "Training ROC AUC: 0.6416741326042006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, scaler,encoder,reference_date = train_pipeline('data/train/accounts_data_train.json', \n",
    "                               'data/train/enquiry_data_train.json', \n",
    "                               'data/train/train_flag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb6cf53-d0ab-4406-b1fd-d9c24ed7bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preprocessing acc data\n",
      "preprocessing enq data\n",
      "aggregation of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing payment history: 100%|████████████████████████████████████████████| 220013/220013 [00:54<00:00, 4062.68it/s]\n",
      "C:\\Users\\CLD\\AppData\\Local\\Temp\\ipykernel_29412\\1234274756.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  closed_accounts['loan_duration']=(closed_accounts['closed_date'] - closed_accounts['open_date']).dt.days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pay hist str processing completed.\n",
      "open acc.\n",
      "closed acc.\n",
      "grouping enq\n"
     ]
    }
   ],
   "source": [
    "test_results = test_pipeline('data/test/accounts_data_test.json', \n",
    "                             'data/test/enquiry_data_test.json', \n",
    "                             'data/test/test_flag.csv', \n",
    "                             model, \n",
    "                             scaler,encoder,reference_date)\n",
    "\n",
    "\n",
    "test_results.to_csv('final_submission_clyde_dcosta.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3fe8c7-6d5f-46e8-9b49-643bb3e5532c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMO22835242</td>\n",
       "      <td>0.504667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MRJ34316727</td>\n",
       "      <td>0.490263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UAV00534378</td>\n",
       "      <td>0.604203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPQ08190402</td>\n",
       "      <td>0.430404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NQN84331006</td>\n",
       "      <td>0.493659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid      pred\n",
       "0  CMO22835242  0.504667\n",
       "1  MRJ34316727  0.490263\n",
       "2  UAV00534378  0.604203\n",
       "3  IPQ08190402  0.430404\n",
       "4  NQN84331006  0.493659"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974ad7be-76da-4d32-b228-7e6210a8cd90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNOTE:\\nFew of the things I tried which are not included in this final code:\\n1. SMOTE: I used this for oversampling of the minority class expecting this to lead to improvement but somehow roc_auc score decreased by ~9 percent.\\n2. using this:\\n\\n def grouping_loans(loan):\\n    if loan in ['Credit card','Revolving loans','Interbank credit']:\\n        return 'Cash loans'\\n    else:\\n        return 'Revolving loans'\\n        \\nI grouped the loan_types under 2 categories as seen in the flag dataframe. \\nI figured that grouping by and then using the specific loan_type as the instance would lead to more relevant information being consumed \\nand hence better predictions (I do this at my current job and it works well). \\nTherefore I had initially grouped by uid and NAME_CONTRACT_TYPE; but this again led to a decrease in accuracy.\\n\\n3.for feature selection I used the following code snippets for quick selection. I usually use RFE and save all the scores in an excel sheet along with the used features and then choose suitable sets. Due to lack of time, I have directly used this here.\\n\\n###for tree based:\\n\\nimportances = model.feature_importances_\\n\\n# Create a DataFrame for feature importances\\nfeature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': importances})\\n\\n# Sort by importance\\nfeature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\\n\\n###for log reg\\n\\ncoefficients = model.coef_[0] \\nfeature_names = X.columns\\n\\nfeature_importance_df = pd.DataFrame({\\n    'feature': feature_names,\\n    'importance': coefficients\\n})\\n\\nfeature_importance_df['absolute_importance'] = feature_importance_df['importance'].abs()\\nfeature_importance_df['percent']=feature_importance_df['absolute_importance']*100/feature_importance_df['absolute_importance'].sum()\\nfeature_importance_df = feature_importance_df.sort_values(by='percent', ascending=False)\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "Few of the things I tried which are not included in this final code:\n",
    "1. SMOTE: I used this for oversampling of the minority class expecting this to lead to improvement but somehow roc_auc score decreased by ~9 percent.\n",
    "2. using this:\n",
    "\n",
    " def grouping_loans(loan):\n",
    "    if loan in ['Credit card','Revolving loans','Interbank credit']:\n",
    "        return 'Cash loans'\n",
    "    else:\n",
    "        return 'Revolving loans'\n",
    "        \n",
    "I grouped the loan_types under 2 categories as seen in the flag dataframe. \n",
    "I figured that grouping by and then using the specific loan_type as the instance would lead to more relevant information being consumed \n",
    "and hence better predictions (I do this at my current job and it works well). \n",
    "Therefore I had initially grouped by uid and NAME_CONTRACT_TYPE; but this again led to a decrease in accuracy.\n",
    "\n",
    "3.for feature selection I used the following code snippets for quick selection. I usually use RFE and save all the scores in an excel sheet along with the used features and then choose suitable sets. Due to lack of time, I have directly used this here.\n",
    "\n",
    "###for tree based:\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "###for log reg\n",
    "\n",
    "coefficients = model.coef_[0] \n",
    "feature_names = X.columns\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': coefficients\n",
    "})\n",
    "\n",
    "feature_importance_df['absolute_importance'] = feature_importance_df['importance'].abs()\n",
    "feature_importance_df['percent']=feature_importance_df['absolute_importance']*100/feature_importance_df['absolute_importance'].sum()\n",
    "feature_importance_df = feature_importance_df.sort_values(by='percent', ascending=False)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5fc2c-8d89-4887-a40f-4a2856861c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
